\documentclass[11pt]{article}
    
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{cite}
\usepackage[letterpaper, total={6.5in, 9in}]{geometry}
\usepackage{mathpazo}
\usepackage{sourcecodepro}

\usepackage{hyperref}

\newcommand{\setcomp}[2]{\left\{ #1 \ \Big|\ #2 \right\}}
\newcommand{\rngto}[1]{1{:}#1}
\newcommand{\abs}[1]{\left| #1 \right|}
\newcommand{\absdet}[1]{\abs{#1}}
\newcommand{\dv}[1]{\mathrm{d}{#1}}
\newcommand{\Exp}{\mathrm{Exp}}
\newcommand{\vect}{\mathrm{vec}}
\newcommand{\vectu}{\mathrm{vecu}}


\begin{document}


\title{Efficient Unconstraining
  Parameter Transforms for Hamiltonian Monte Carlo}
\author{Meenal Jhajaria \\ \small Flatiron Institute \and Seth Axen \\
  \small University of T\"ubingen \and Bob
  Carpenter \\ \small Flatiron Institute}
\date{DRAFT: \today}
\maketitle


\begin{abstract}
  \noindent
  This paper evaluates the the statistical and computational
  efficiency of unconstraining parameter transforms for Hamiltonian
  Monte Carlo sampling.
\end{abstract}

\section{Introduction}

In statistical computing, we often need to compute high-dimensional
integrals over densities $\pi(x)$ (e.g., Bayesian estimation or
prediction, $p$-value calcuations, etc.).  The only black-box
techniques that work for general high-dimensional integrals are
Markov chain Monte Carlo (MCMC) methods.  The most effective MCMC
method in high dimensions is Hamiltonian Monte Carlo (HMC).  HMC works
by simulating the Hamiltonian dynamics of a fictitious particle
representing the value being sampled coupled with a momentum term.

Although it's possible to write HMC samplers that work for simply
constrained values such as upper- and/or lower-bounds
\cite{neal2011mcmc} or unit vectors \cite{byrne2013geodesic}, it is
far more challenging to do the same for complex constraints such as
simplices or positive definite matrices or for densities involving
multiple constrained values.  Instead, usually constrained values are mapped to unconstrained values before sampling. These transform mappings are formed out of the constraints on the model parameters. A change of variables adjustment is made to the target density for these transforms to work. The tricky part is- there can be more than one suitable mapping for every constraint. This leads us to the issue of selecting a transform. 

We evaluate the efficiency of unconstraining parameter transforms using metrics like effective sample size, leapfrog steps and squared error. Different transforms work well for different models, this paper aims to provide a useful analysis of the same, so practitioners can choose transforms that work well for their models.

\section{Transforms}
The transform from a constrained space $\mathcal{X}$ to an unconstrained space $\mathcal{Y}$ is a smooth and bijective function $f: \mathcal{X} \to \mathcal{Y}$. We aim to induce a density $h(y)$ on $\mathcal{Y}$ and define a smooth and continous map $g\colon \mathcal{Y} \to \mathcal{X}$, such that $h(y)$ is uniform over $\mathcal{X}$. 
%review and make this better
Uniformity is defined here to mean, if $y$, $y`$ $\sim$ $h(y)$,  then $g(y)$, $g(y`)$ $\sim$ uniform($\mathcal{X}$).	


% check m,n relationship
%To draw samples from a $y$ $p_X(x)$, defined on a constrained space $\mathcal{X}$ that can be uniquely parameterized by $n$ real degrees of freedom, we instead perform sampling in an unconstrained space $\mathcal{Y}=\mathbb{R}^m$ for $m \ge n$.


%Only when $m = n$ can $f$ be bijective.
%When $m > n$, multiple values of $y \in \mathcal{Y}$ may map to a given value of $x = g(y)$.

%In this case, to make the function bijective, we consider an additional smooth and continuous map $g\colon \mathcal{Y} \to \mathcal{Z}$, where $\mathcal{Z}$ can be uniquely parameterized by $m - n$ real degrees of freedom.
%We then define $h\colon \mathcal{Y} \to \mathcal{X} \times \mathcal{Z}: y \mapsto (f(y), g(y)) = (x, z)$.
%$f$ and $g$ must be chosen so that $h$ is bijective almost-everywhere;
%that is, the values of $y$ for which $h$ is non-bijective must have no probability mass.
%When $h$ is non-bijective at single points, we call these singularities.
%While singularities themselves are in general not a problem for sampling with MCMC, the region of a target density near the singularity tends to have high curvature.



\subsection{Changes of Variables}

If $Y \in \mathbb{R^N}$ is a random variable with density $p_Y(y)$, where $Y = g^{-1}(X)$, where $X \in \mathbb{R^M}$. Then for a smooth and bijective function $g$ which is the inverse transform from an unconstrained space to a constrained space, we can define its density as:
\[
  p_Y(y) = p_X(g(y)) \absdet{J_{g}(y)},
\]
where the Jacobian of the inverse transform is defined by
\[
  J_{g}(y) = \frac{\partial}{\partial y} \, g(y)
\]
and
\[
  \absdet{J_{g}(y)}
  = \abs{\det J_{g}(y)}.
\]
\section{Unit simplex}

A unit $N$-simplex is an $N + 1$-dimensional vector of non-negative
values that sums to one.  As such, there are only $N$ degrees of
freedom, because if $x$ is an $N$-simplex, then
\[
  x_N = -(x_1 + x_2 + \cdots + x_{N-1}).
\]
Simplexes are useful for representations of multinomial probabilities
(e.g., probabilities of categories in a classification problem).

The set of unit $N$-simplexes is conventionally denoted
\[
  \Delta^N = \setcomp{x \in \mathbb{R}^{N + 1}}{\textrm{sum}(x) = 1
    \textrm{ and }
    x_n \geq 0 \textrm{ for } n \in \rngto{N}}
\]
Geometrically, an $N$-simplex is the convex closure of $N+1$ points
that are 1 in one coordinate and 0 elsewhere.  For example, the
3-simplex is the complex closure of
$\begin{bmatrix}1 & 0 & 0 \end{bmatrix},
\begin{bmatrix} 0 & 1 & 0 \end{bmatrix}$,
and $\begin{bmatrix} 0 & 0 & 1 \end{bmatrix}$.

\subsection{StickBreaking Transform}

The StickBreaking transform can be be understood from the stick-breaking construction for Dirichlet \cite{sethurman}. Intuitively, this comprises of recursively breaking a piece $x_i$ from a stick of unit length, where the leftover stick in the $i^{th}$ iteration is $ 1 - \sum_{1}^{i}x$. Let $y = f(x)$, then we define the stick-breaking mapping $ f \colon \Delta^{N-1} \xrightarrow{\makebox[0.4cm]{$\sim$}}  R^{N-1}$, for $1 \leq i \leq N$ as:	
\[
y_i
= \mathrm{logit}(z_i) - \mbox{log}\left(\frac{1}{N-i}
   \right) \text{for break proportion} \, 
   z_i = \frac{x_i}{1 - \sum_{i' = 1}^{i-1} x_{i'}}.
\]

The inverse transform $ f^{-1} \colon R^{N-1} \xrightarrow{\makebox[0.4cm]{$\sim$}}  \Delta^{N-1}$ is defined as:
\[
x_i =
\left( 1 - \sum_{i'=1}^{i-1} x_{i'} \right) \text{for break proportion} \, z_i = \mathrm{logit}^{-1} \left( y_i
                             + \log \left( \frac{1}{N - i}
                                            \right)\right).
                                            \]
                            
An $N$ dimensional unit simplex $\Delta^{N-1}$ has $N-1$ degrees of freedom (Notice that this inverse transform only maps the first $N-1$ elements, the last element of the simplex $x_{N} = 1 - \sum_1^{N-1}{x_i}$). The Jacobian matrix for $f^{-1}$ is a lower-triangular diagonal matrix. Much like the alr transform, for the change of variables we evaluate $\mathbf{J}_{i, i}$ where $i \in 1:N-1$.
\begin{align*}
\mathbf{J}_{i, i} &= \frac{\partial x_i}{\partial y_i}
=
\frac{\partial x_i}{\partial z_i} \,
\frac{\partial z_i}{\partial y_i}\\
\mathbf{J}_{i, i} &= \left(
  1 - \sum_{k' = 1}^{k-1} x_{k'}
   \right) z_k (1 - z_k),
\end{align*}

Absolute determinant of the diagonal matrix $\mathbf{J}$ is the product of its diagonal entries:
\begin{align*}
	\abs{\, det \, \textbf{J} \,} = \prod_{i=1}^{N-1} \textbf{J}_{i,i}
\end{align*}

The correction term $p_Y(y) = p_X(f^{-1}(y))\,
\prod_{i=1}^{N-1}z_i\,(1 - z_i)\left(1 - \sum_{i'=1}^{i-1} x_{i'}\right).$
\subsection{Additive log ratio transform}

The unconstraining transform for the identified softmax is known as
the additive log ratio (ALR) transform
\cite{aitchison1982statistical}, which is a bijection
$\textrm{alr}:\Delta^{N-1} \rightarrow \mathbb{R}^{N-1}$ defined for
$x \in \Delta^{N-1}$ by
\[
  \textrm{alr}(x)
  = \begin{bmatrix}\displaystyle
    \log \frac{x_1}{x_N} \cdots \log \frac{x_{N-1}}{x_N}
  \end{bmatrix}.
\]

The inverse additive log ratio transform maps values in
$\mathbb{R}^{N-1}$ to $\Delta^{N-1}$ defined for $y \in
\mathbb{R}^{N-1}$ by
\[
  \textrm{alr}^{-1}(y)
  = \textrm{softmax}(\begin{bmatrix} y &  0 \end{bmatrix}),
\]
where for $u \in \mathbb{R}^N$,
\[
  \textrm{softmax}(u) = \frac{\exp(u)}{\textrm{sum}(\exp(u))}.
\]

To calculate the change of variables adjustment, we consider only the
first $N-1$ coordinates of the result, because the last is defined in
terms of the first $N-1$.  For convenience, we define a function 
$s:\mathbb{R}^{N-1} \rightarrow \mathbb{R}^{N-1}$ that operates on
$N-1$ unconstrained variables and returns the first $N-1$ components
of the $\textrm{alr}^{-1}(y)$, which is
defined for $y \in \mathbb{R}^{N-1}$ by
\[
  s(y) = \frac{\exp(y)}{\textrm{sum}(\exp(y)) + 1}
  = \begin{bmatrix}
    \textrm{alr}^{-1}(y)_1
    & \cdots &
    \textrm{alr}^{-1}(y)_{N-1}
    \end{bmatrix}.
\]
Given a density $p_X(x)$ defined over simplices $x \in \Delta^{N-1}$,
we can transform to a density over unconstrained parameters $y \in
\mathbb{R}^{N-1}$ by applying the inverse ALR transform and adjusting
for the change of variables, which yields
\[
  p_Y(y) = p_X(\textrm{alr}^{-1}(y)) \absdet{J_{s}(y)},
\]
where $J_{s}(y)$ is the Jacobian of the function $s$ evaluated at $y$
and $\absdet{J_s(y)}$ is the absolute value of its determinant.


\subsubsection{Softmax Transform}

To calculate the determinant of the Jacobian of the inverse transform,
we start by noting that $s = \textrm{exp} \circ \textrm{norm}$, where
$\textrm{exp}$ is the elementwise exponential function and
\textrm{norm} is defined by
\[
  \textrm{norm}(z) = \frac{z}{\textrm{sum}(z) + 1}.
\]
As such, the resulting Jacobian determinant is the product of the
Jacobian determinants of the component functions,
\[
  \absdet{J_s(y)}
  = \absdet{J_{\textrm{exp}}(y)} \absdet{J_{\textrm{norm}}(z)},
\]
where $z = \textrm{exp}(y)$.  The Jacobian for the exponential
function is diagonal, so the determinant is the product of the
diagonal of the Jacobian, which for $y \in \mathbb{R}^{N-1}$ is
\[
  \absdet{J_{\textrm{exp}}(y)} = \textrm{prod}(\exp(y)).
\]
As above, let $z = \exp(y) \in (0, \inf)^{N-1}$.  We can differentiate
$\textrm{norm}$ to derive the Jacobian,
\[
  J_{\textrm{norm}}
  = \frac{1}{1 + \textrm{sum}(z)} \mathbb{I}_{N-1}
  - \left(\frac{1}{(1 + \textrm{sum}(z))^2} \beta \right)
  \textrm{vector}_{N-1}(1)^{\top},
\]
where $\mathbb{I}_{N-1}$ is the $(N - 1) \times (N - 1)$ unit matrix and
$\textrm{vector}_{N-1}(1)$ is the $N - 1$-vector with values 1.  Using
the matrix determinant lemma,\footnote{The matrix determinant lemma
  is \[\textrm{det}(A + u v^{\top}) = (1 + v^{\top} A^{-1} u)
    \textrm{det}(A).\]}
we have
\begin{eqnarray*}
  \textrm{absdet}(J_{\textrm{norm}}(z))
  & = &
  \left(
    1
    + \textrm{vector}_{N-1}(1)^{\top}
    \left(\frac{1}{1 + \textrm{sum}(z)} \mathbb{I} \right)^{-1}
    \frac{-z}{(1 + \textrm{sum}(z))^2}
    \right)
    \ \textrm{det}\left(\frac{1}{1 + \textrm{sum}(z)} \mathbb{I}
        \right)
  \\[6pt]
  & = & \left( \frac{1}{1 + \textrm{sum}(z)} \right)^N.
\end{eqnarray*}
Thus the entire absolute determinant of the Jacobian is defined by the
product, 
\[
  \absdet{J_s(y)}
  \ = \
  \textrm{prod}(\exp(y))
  \, \left( \frac{1}{1 + \textrm{sum}(\exp(y))} \right)^N.
\]
and our final expression for densities for unconstrained $y \in
\mathbb{R}^{N-1}$ is
\[
  p_Y(y)
  = p_X(\textrm{alr}^{-1}(y))
  \, \textrm{prod}(\exp(y))
  \left( \frac{1}{1 + \textrm{sum}(\exp(y))} \right)^N
\]  



\subsection{Simplex augmented softmax parameterization}

We define the transformation
$\phi: \mathbb{R}^n \to \Delta^{n-1} \times \mathbb{R}_{>0}: y \mapsto
(x_-, r)$, where $r = \sum_{i=1}^n e^{y_i}$ and
$x_i = \frac{1}{r} e^{y_i}$ for $1 \le i \le n-1$..

First we compute the scalar derivatives:
\[
\begin{aligned}
  \frac{\mathrm{d} r}{\mathrm{d} y_j}
  &= e^{y_j} = r x_j
  \\
  \frac{\mathrm{d} x_i}{\mathrm{d} y_j}
  &= \delta_{ij} \frac{1}{r} e^{y_i} - \frac{1}{r^2} e^{y_i} \frac{\mathrm{d} r}{\mathrm{d} y_j} = \delta_{ij} x_i - x_i x_j,
\end{aligned}
\]
which corresponds to the Jacobian
\[
  J = \begin{pmatrix}I_{n-1} - x_- \boldsymbol{1}_{n-1}^\top & -x_- \\
    r \boldsymbol{1}_{n-1}^\top & r \end{pmatrix} \mathrm{diag}(x).
\]

For invertible $A$, the determinant of the block matrix
$\begin{pmatrix}A & B \\ C & D\end{pmatrix}$ is $|A| |D-CA^{-1}B|$.  A
square matrix is invertible iff its determinant is non-zero.  From the
previous section,
\[
  |I_{n-1} - x_- \boldsymbol{1}_{n-1}^\top| = x_n > 0,
\]
so the determinant of the Jacobian is
\[
  |J| = x_n \left|r + r \boldsymbol{1}_{n-1}^\top (I_{n-1} - x_-
    \boldsymbol{1}_{n-1}^\top)^{-1} x_-\right|
  \prod_{i=1}^n x_i.
\]

Let $w = (I_{n-1} - x_- \boldsymbol{1}_{n-1}^\top)^{-1} x_-$. Then,
\[
\begin{aligned}
    w - x_- \sum_{i=1}^{n-1} w_i &= x_-\\
    w &= x_- \left(1 - \sum_{i=1}^{n-1} w_i\right)\\
    \sum_{i=1}^{n-1} w_i &= \sum_{i=1}^{n-1} \left( x_- (1 - \sum_{i=1}^{n-1} w_i) \right) = \left(\sum_{i=1}^{n-1} x_i \right) \left(1 - \sum_{i=1}^{n-1} w_i\right) = (1 - x_n)  \left(1 - \sum_{i=1}^{n-1} w_i\right)\\
    \sum_{i=1}^{n-1} w_i &= \frac{1 - x_n}{x_n} = \frac{1}{x_n} - 1\\
    w &= x_- \left(1 - \left(\frac{1}{x_n} - 1\right)\right) = \frac{1}{x_n} x_-
  \end{aligned}
\]

Then
\[
  |J| = x_n r \left|1 + \frac{1}{x_n}\sum_{i=1}^{n-1} x_i\right|
  \prod_{i=1}^n x_i = r \prod_{i=1}^n x_i
\]

To keep the target distribution proper, we must select a prior
distribution $\pi(r)$ for $r$.  If we choose $r \sim \chi_n$, then the
product of the correction and the density of the prior for $r$ is
proportional to

\[
  \mathrm{correction}
  = \pi(r) |J| = r^n e^{-r^2/2} \prod_{i=1}^n x_i
  = \exp\left(\sum_{i=1}^n y_i - \frac{1}{2}\left(\sum_{i=1}^n
      e^{y_i}\right)^2\right).
\]

Alternatively, if we choose $r \sim \mathrm{Gamma}(n, 1)$, then
\[
  \mathrm{correction} = \pi(r) |J| = r^n e^{-r} \prod_{i=1}^n x_i =
  \exp\left(\sum_{i=1}^n y_i - \sum_{i=1}^n e^{y_i}\right).
\]
This latter correction is equivalent to the sampling procedure from
the Dirichlet distribution with $\alpha_i=1$, where
$z_i \sim \mathrm{Exponential}(1)$ and
$y = \frac{z}{\sum_{i=1}^n z_i}$.

Both of these corrections can be captured with the generalization
\[
  \mathrm{correction}
  = \pi(r) |J|
  = r^n e^{-r^p/p} \prod_{i=1}^n x_i
  = \exp\left(\sum_{i=1}^n y_i - \frac{1}{p} \left(\sum_{i=1}^n e^{y_i}\right)^p\right),
\]
for $p > 0$, which corresponds to $r \sim \text{Generalized-Gamma}(1, n, p)$.
\subsection*{Acknowledgements}

We would like to thank \url{matrixcalculus.org} for providing an
easy-to-use symbolic matrix derivative calculator.



\bibliography{all}{}
\bibliographystyle{plain}

\end{document}
